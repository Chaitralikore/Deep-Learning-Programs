{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9p7fhNwpScO",
        "outputId": "bf17174b-f40d-4f1c-da9e-0c885b60783b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Encoded Review:\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "\n",
            "Decoded Review:\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "\n",
            "Sentiment: Positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 377ms/step - accuracy: 0.5236 - loss: 0.6866 - val_accuracy: 0.5942 - val_loss: 0.6516\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 378ms/step - accuracy: 0.6393 - loss: 0.6322 - val_accuracy: 0.6930 - val_loss: 0.5961\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 383ms/step - accuracy: 0.6639 - loss: 0.5941 - val_accuracy: 0.6284 - val_loss: 0.6189\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.6286 - loss: 0.6143\n",
            "\n",
            "Test Accuracy: 0.6241999864578247\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense     #The Embedding Layer translates words into meaningful number profiles so the LSTM can \"understand\" language\n",
        "\n",
        "# Dataset parameters\n",
        "vocab_size = 10000  # Use top 10K frequent words, rare words are replaced with <UNK>,    Balances model complexity and computational efficiency.\n",
        "maxlen = 200        # padding: maxlen=200 ensures all reviews are 200 words long.\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Create word-to-index mapping\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Decoding setup (for human-readable output)\n",
        "reverse_word_index = {value + 3: key for key, value in word_index.items()}\n",
        "reverse_word_index[0] = \"<PAD>\"     # Padding token\n",
        "reverse_word_index[1] = \"<START>\"   # Sequence start token\n",
        "reverse_word_index[2] = \"<UNK>\"     # Unknown word token\n",
        "reverse_word_index[3] = \"<UNUSED>\"  # Reserved token\n",
        "\n",
        "# Decode integer sequence to text\n",
        "def decode_review(encoded_review):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
        "\n",
        "# Display sample review\n",
        "sample_index = 0\n",
        "print(\"Decoded Review Example:\")\n",
        "print(decode_review(x_train[sample_index]))\n",
        "print(\"Sentiment:\", \"Positive\" if y_train[sample_index] == 1 else \"Negative\")\n",
        "\n",
        "# Pad sequences to uniform length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=maxlen),  # Word embeddings\n",
        "    LSTM(100),                                                           # LSTM layer     A single LSTM layer with 100 memory units (neurons).\n",
        "    Dense(1, activation='sigmoid')                                       # Output layer\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',   # Binary classification loss, Measures the difference between predicted probabilities and true labels (0/1)\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(x_train, y_train,\n",
        "                   epochs=3,\n",
        "                   batch_size=64,\n",
        "                   validation_split=0.2)    #Reserves 20% of training data for validation (monitors overfitting).\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"\\nTest Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#To perform binary sentiment classification (positive/negative) on IMDB movie reviews using an LSTM-based deep learning model, demonstrating NLP (Natural Language Processing) for text analysis.\n",
        "\n",
        "#Sigmoid activation:     Sigmoid outputs a value between 0 (negative) and 1 (positive).\n",
        "                        #Ideal for binary classification (vs. softmax for multi-class).\n",
        "\n",
        "# How would you improve this model?\n",
        "        #Add Dropout (e.g., Dropout(0.2)) to prevent overfitting.\n",
        "\n",
        "#limitations of this approach:   Fixed vocabulary: Rare words are ignored (<UNK>)\n",
        "\n",
        "# purpose of pad_sequences:   Ensures all reviews have the same length (maxlen).\n",
        "                             #Padding: Adds zeros to shorter sequences.\n",
        "                              #Truncation: Cuts off excess words in longer sequences.\n",
        "\n",
        "#word_index dictionary do:  Maps each word to a unique integer (e.g., {\"movie\": 17, \"good\": 42}).\n",
        "                            #Used to encode reviews as integer sequences.\n",
        "\n",
        "#Epochs\t3\tTraining iterations\n",
        "#The encoded review ([1, 14, 22, ...]) is the numerical version of the text, where:\n",
        "#Each number = a word (e.g., 14 = \"this\", 22 = \"film\").\n",
        "#1 = Start, 2 = Unknown word, 0 = Padding (to make all reviews the same length).\n",
        "\n",
        "# How was the \"Decoded Review\" generated?\n",
        "         #Mapped integer IDs back to words using reverse_word_index\n",
        "        #Replaced special tokens:\n",
        "         #<START> = Beginning of review\n",
        "         #<UNK> = Unknown word (not in top 10,000 vocabulary)\n",
        "\n",
        "# UserWarning about input_length:     Modern Keras versions automatically handle sequence length\n",
        "\n",
        "#Why did validation accuracy drop in Epoch 3?\n",
        "            #Overfitting: Model memorized training data (solution: add Dropout)\n",
        "            #Small dataset: Only 3 epochs may not show stable trends\n",
        "            #Learning rate: Too high\n",
        "\n",
        "#final Test Accuracy (62.42%) mean:\n",
        "                            #The model correctly classified 62.42% of unseen reviews\n",
        "                            #Baseline for binary classification: 50% (random guessing)\n",
        "                            #Interpretation: The LSTM learned some sentiment patterns but has room for improvement\n",
        "\n",
        "#mprove 62.42% accuracy?   :  Increase vocabulary size (from 10,000 to 20,000 words)    , Add Dropout layers (e.g., Dropout(0.2) after LSTM)\n",
        "\n",
        "#timing information (e.g., 122s/epoch:  Training time per epoch\n"
      ]
    }
  ]
}