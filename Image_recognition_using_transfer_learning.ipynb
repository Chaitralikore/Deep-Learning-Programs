{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeo1rl7r7b4VJWfPflnd/4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import zipfile\n","import os\n","import urllib.request"],"metadata":{"id":"Qk6OZTSpubNk","executionInfo":{"status":"ok","timestamp":1747128265547,"user_tz":-330,"elapsed":16,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Step 1: Download and extract the dataset\n","url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n","data_dir = \"cats_and_dogs_filtered\"\n","if not os.path.exists(data_dir):\n","    urllib.request.urlretrieve(url, \"cats_and_dogs_filtered.zip\")\n","    with zipfile.ZipFile(\"cats_and_dogs_filtered.zip\", 'r') as zip_ref:\n","        zip_ref.extractall()"],"metadata":{"id":"kobGHlkQ9beM","executionInfo":{"status":"ok","timestamp":1747128295105,"user_tz":-330,"elapsed":1447,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Paths for training and validation data\n","train_dir = os.path.join(data_dir, \"train\")\n","validation_dir = os.path.join(data_dir, \"validation\")"],"metadata":{"id":"MV15MHdg9kxn","executionInfo":{"status":"ok","timestamp":1747128313657,"user_tz":-330,"elapsed":9,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Step 2: Data Preprocessing\n","train_datagen = ImageDataGenerator(       #It preprocesses images and applies augmentation to enhance model generalization.\n","    rescale=1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n",")\n","validation_datagen = ImageDataGenerator(rescale=1./255)   #It normalizes pixel values to the range [0, 1], improving model performance.\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary'\n",")\n","validation_generator = validation_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsPf1gLi9sgf","executionInfo":{"status":"ok","timestamp":1747128343122,"user_tz":-330,"elapsed":30,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"f3d65816-1b46-4dc7-bc2d-8d636b3fb75a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["# Step 3: Load Pre-trained Model\n","base_model = MobileNetV2(weights='imagenet', include_top=False)       #It excludes the fully connected layers, allowing customization for the new task.\n","\n","# Freeze the base model\n","base_model.trainable = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mauGbqGq9xOI","executionInfo":{"status":"ok","timestamp":1747128363151,"user_tz":-330,"elapsed":1221,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"148b03c8-c865-4775-a95a-dd7a04ab6c21"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-518d74350156>:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","  base_model = MobileNetV2(weights='imagenet', include_top=False)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["# Step 4: Build the Model\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),             #It reduces the feature map dimensions by averaging, making the model more efficient.\n","    Dropout(0.2),\n","    Dense(1, activation='sigmoid')        #For binary classification, sigmoid outputs probabilities for each class.\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"KEwHCHeG93qe","executionInfo":{"status":"ok","timestamp":1747128388351,"user_tz":-330,"elapsed":50,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Step 5: Train the Model\n","history = model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=5\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUuK0H1B979L","executionInfo":{"status":"ok","timestamp":1747129117287,"user_tz":-330,"elapsed":711209,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"6a9b67a2-6671-4105-b6b0-80c1f311be6a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2s/step - accuracy: 0.7884 - loss: 0.4411 - val_accuracy: 0.9790 - val_loss: 0.1028\n","Epoch 2/5\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.9302 - loss: 0.1716 - val_accuracy: 0.9770 - val_loss: 0.0763\n","Epoch 3/5\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.9431 - loss: 0.1408 - val_accuracy: 0.9750 - val_loss: 0.0792\n","Epoch 4/5\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - accuracy: 0.9424 - loss: 0.1380 - val_accuracy: 0.9790 - val_loss: 0.0639\n","Epoch 5/5\n","\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - accuracy: 0.9591 - loss: 0.1175 - val_accuracy: 0.9800 - val_loss: 0.0649\n"]}]},{"cell_type":"code","source":["# Step 6: Evaluate the Model\n","loss, accuracy = model.evaluate(validation_generator)\n","print(f\"Validation Accuracy: {accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYOHvqxC-JrU","executionInfo":{"status":"ok","timestamp":1747129295796,"user_tz":-330,"elapsed":41384,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"dda0155e-24d0-40e4-ddd5-5ef6b38a899b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.9813 - loss: 0.0612\n","Validation Accuracy: 0.98\n"]}]},{"cell_type":"code","source":["# Save the model\n","model.save('cat_dog_recognition_model.h5')\n","print(\"Model saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbgBfGmKBpW9","executionInfo":{"status":"ok","timestamp":1747129379063,"user_tz":-330,"elapsed":291,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"fc352ce7-f99b-4738-86f8-069e4b0455bd"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model saved successfully!\n"]}]},{"cell_type":"code","source":["#Classes Loaded: Successfully loaded 2000 training images and 1000 validation images, belonging to 2 classes (Cats and Dogs).\n","#MobileNetV2 Warning: The warning about input_shape is harmless since the default input shape (224, 224) was loaded correctly.\n","#Training Results:\n","#Achieved 98% validation accuracy after 5 epochs.\n","#Loss values decreased steadily, indicating good learning progress.\n","#Model Saved: The model was saved successfully in HDF5 format, but you might consider saving in the newer .keras format as suggested.\n","\n","#conclusion:     Achieved High Accuracy: The model successfully achieved a validation accuracy of 98%, indicating excellent performance in distinguishing between cats and dogs.\n","                #Efficient Use of Transfer Learning: By leveraging the pre-trained MobileNetV2, the program demonstrated how transfer learning can save time and resources while maintaining high accuracy.\n","                 #Robust Pipeline: The inclusion of data augmentation and normalization ensured that the model generalized well to unseen data.\n","                 #Reusable Model: The trained model was saved, making it ready for future use without the need to retrain."],"metadata":{"id":"34T5m_TwBwLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Transfer learning is a technique where a pre-trained model is adapted to solve a new problem, leveraging its existing knowledge.\n","\n","#MobileNetV2 is lightweight, efficient, and optimized for mobile and edge devices while retaining high accuracy.\n","\n","#advantages of transfer learning?\n","               #Faster training, reduced data requirements, and leveraging pre-trained features for better accuracy.\n","\n","#What kind of problem is being solved in this program?\n","                    #Binary classification for recognizing images of cats and dogs."],"metadata":{"id":"WeV20b4SB0Yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#What loss function is used here, and why?\n","       #Binary crossentropy, as it’s suitable for binary classification problems.\n","#What does the accuracy metric indicate during training?\n","       #It measures the proportion of correctly classified images.\n","#Why do we use validation data during training?\n","        #To monitor model performance and avoid overfitting.\n","#What does the training history (loss and accuracy trends) tell us?\n","         #It shows how well the model is learning and generalizing over epochs.\n","\n","#Why might the validation accuracy differ from training accuracy?\n","         #Due to overfitting, data distribution differences, or noise in the validation set.\n","\n","#How would you handle overfitting if it occurred?\n","         #By using techniques like dropout, more data augmentation, or reducing model complexity.\n","\n","# evaluate() function do?\n","        #It computes the loss and accuracy on the given dataset."],"metadata":{"id":"qrD_JxwIEo-5"},"execution_count":null,"outputs":[]}]}