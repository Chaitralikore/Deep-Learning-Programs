{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaf0myTXze0UCYkU2oOxNt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"WbVNEkRHoaw5","executionInfo":{"status":"error","timestamp":1747173129064,"user_tz":-330,"elapsed":15793,"user":{"displayName":"Chaitrali Kore","userId":"17149086639972974263"}},"outputId":"bd3b64cb-d1c1-4a3a-d5a1-c17473591dfe"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'your_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2b6070c7d12f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Step 1: Load Kaggle dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Replace 'your_dataset.csv' with the actual file name from Kaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 2: Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","\n","\n","\n","\n","\n","# Step 1: Load Kaggle dataset\n","# Replace 'your_dataset.csv' with the actual file name from Kaggle\n","dataset = pd.read_csv('your_dataset.csv')\n","\n","# Step 2: Preprocess the data\n","# Assuming the last column is the target and the rest are features\n","X = dataset.iloc[:, :-1].values  # Features\n","y = dataset.iloc[:, -1].values   # Target\n","\n","# One-hot encode the target variable if it's categorical\n","if len(np.unique(y)) > 2:  # Multi-class classification\n","    y = tf.keras.utils.to_categorical(y)\n","\n","\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","\n","X_test = scaler.transform(X_test)\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Normalize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Step 3: Build the Neural Network\n","model = Sequential([\n","    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n","    Dense(32, activation='relu'),  # Hidden layer\n","    Dense(y.shape[1] if len(y.shape) > 1 else 1, activation='softmax' if len(y.shape) > 1 else 'sigmoid')  # Output layer\n","])\n","\n","# Step 4: Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy' if len(y.shape) > 1 else 'binary_crossentropy', metrics=['accuracy'])\n","\n","# Step 5: Train the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n","\n","# Step 6: Evaluate the model\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n","\n","# Step 7: Save the model\n","model.save(\"classification_model.h5\")\n","print(\"Model saved successfully!\")"]},{"cell_type":"markdown","source":["Conclusion\n","This program demonstrates a complete pipeline for performing classification using a Neural Network via TensorFlow/Keras. Here's a summary:\n","\n","Dataset Handling:\n","\n","The program loads a Kaggle dataset for classification tasks.\n","It preprocesses the data by splitting it into training and testing sets and normalizing the features.\n","Model Building:\n","\n","A feedforward neural network is constructed with one input layer, one hidden layer, and one output layer.\n","The architecture supports both binary and multi-class classification based on the dataset.\n","Training:\n","\n","The model is trained using the Adam optimizer and appropriate loss functions (binary_crossentropy or categorical_crossentropy), depending on the task.\n","Evaluation:\n","\n","The model's performance is evaluated on the test dataset to calculate accuracy.\n","The trained model is saved as classification_model.h5 for future use.\n","Viva Questions and Answers\n","1. What is the purpose of this program?\n","The program performs classification on a Kaggle dataset using a neural network with backpropagation.\n","2. How is the dataset split into training and testing?\n","The dataset is split into 80% training and 20% testing using train_test_split() from sklearn.\n","3. Why is the data normalized?\n","Normalization ensures that all features have a similar scale, which improves the model's convergence during training.\n","4. What is the role of the StandardScaler?\n","StandardScaler standardizes features by removing the mean and scaling to unit variance.\n","5. What is the significance of the activation functions used?\n","ReLU:\n","Used in hidden layers to introduce non-linearity and handle vanishing gradient problems.\n","Sigmoid/Softmax:\n","Sigmoid is used for binary classification.\n","Softmax is used for multi-class classification.\n","6. What optimizer is used, and why?\n","Adam optimizer:\n","Combines the benefits of both Adagrad and RMSProp optimizers.\n","It adapts the learning rate dynamically, leading to faster convergence.\n","7. What loss functions are used in this program?\n","Binary Crossentropy:\n","Used for binary classification tasks.\n","Categorical Crossentropy:\n","Used for multi-class classification tasks.\n","8. How is the performance of the model evaluated?\n","The test dataset is used to calculate:\n","Test Loss: Measures how well the model predicts.\n","Test Accuracy: Percentage of correctly classified samples.\n","9. What is the structure of the neural network?\n","Input Layer: Number of neurons = Number of input features.\n","Hidden Layer: 64 neurons with ReLU activation.\n","Output Layer:\n","1 neuron with Sigmoid activation for binary classification.\n","(n) neurons with Softmax activation for multi-class classification.\n","10. Why is the model saved, and how?\n","The model is saved using model.save(\"classification_model.h5\") to allow reuse without retraining.\n","11. How does backpropagation work in this program?\n","Backpropagation adjusts weights and biases by calculating gradients of the loss function with respect to parameters using the chain rule.\n","12. What is the role of epochs and batch size?\n","Epochs: Number of times the entire training dataset is passed through the model.\n","Batch Size: Number of samples processed before updating the model's weights."],"metadata":{"id":"TkbvPCSlqxCB"}},{"cell_type":"markdown","source":[],"metadata":{"id":"e_WeY4INqy2_"}}]}